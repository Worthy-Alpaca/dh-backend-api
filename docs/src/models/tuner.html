<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.models.tuner API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.models.tuner</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import optuna
import os
import pickle
import torch.nn as nn
from pathlib import Path
from typing import Literal, Union
from os.path import exists

try:
    from trainModel import TrainModel
    from helper.model import Network
    import helper.losses as losses
except:
    from src.models.trainModel import TrainModel
    from src.models.helper.model import Network
    import src.models.helper.losses as losses

PATH = Path(os.getcwd() + os.path.normpath(&#34;/data/models&#34;))


class Tuner:
    def __init__(
        self,
        dataPath: Path,
        epochs: int,
        direction: Literal[&#34;minimize&#34;, &#34;maximize&#34;] = &#34;maximize&#34;,
        sampler: optuna.samplers = optuna.samplers.TPESampler,
        pruner: optuna.pruners = optuna.pruners.HyperbandPruner,
    ) -&gt; optuna.study:
        &#34;&#34;&#34;Class to initiate a Model tuning session.

        Args:
            dataPath (Path): Path to a data source. May change to DB connection
            epochs (int): Number of epochs in each Trial run
            direction (Literal[&amp;quot;minimize&amp;quot;, &amp;quot;maximize&amp;quot;], optional): Direction to optimize. &amp;quot;minimize&amp;quot; optimizes Loss,  &amp;quot;maximize&amp;quot; optimizes Accuracy. Defaults to &#34;maximize&#34;.
            sampler (optuna.samplers, optional): Optuna Sampler Algorythm to use. Defaults to optuna.samplers.TPESampler.
            pruner (optuna.pruners, optional): Optuna Pruner Algorythm to use. Defaults to optuna.pruners.HyperbandPruner.

        Returns:
            optuna.study: Optuna Study like session.
        &#34;&#34;&#34;
        # assigning variables
        self.dataPath = dataPath
        self.epochs = epochs
        self.direction = direction
        # create study
        self.study = optuna.create_study(
            direction=direction,
            sampler=sampler(),
            pruner=pruner(),
            storage=&#34;sqlite:///50Studies_p4.db&#34;,
        )

    def optimize(
        self,
        n_trials: int,
    ) -&gt; optuna.trial.FrozenTrial:
        &#34;&#34;&#34;Starts the optimization process.

        Args:
            n_trials (int): Number of trials to run.

        Returns:
            optuna.trial.FrozenTrial: The best trial according to optimizer.
        &#34;&#34;&#34;
        print(
            f&#34;Starting optimizer session with {n_trials} Trials and {self.epochs} Epochs&#34;
        )
        self.study.optimize(
            self.__objective,
            n_trials=n_trials,
            catch=(RuntimeError, RuntimeWarning, TypeError, ValueError),
            callbacks=[self.__logging_callback],
        )

        return self.study.best_trial

    def __objective(self, trial: optuna.trial) -&gt; float:
        &#34;&#34;&#34;Generate the study objectives.

        Args:
            trial (optuna.trial): The current trial.

        Raises:
            TypeError: Unknown Direction. Shouldn&#39;t occur. Sanity check.

        Returns:
            float: The current objective. Either accuracy or loss
        &#34;&#34;&#34;
        params = {
            &#34;n_layers&#34;: trial.suggest_int(&#34;n_layers&#34;, 1, 5),
            &#34;n_units_layers&#34;: [],
            &#34;learning_rate&#34;: trial.suggest_loguniform(&#34;learning_rate&#34;, 1e-6, 9e-3),
            &#34;optimizer&#34;: trial.suggest_categorical(
                &#34;optimizer&#34;,
                [&#34;Adamax&#34;],  # &#34;ASGD&#34;, &#34;Adam&#34;, &#34;Adamax&#34;]
            ),
            &#34;scale_data&#34;: trial.suggest_categorical(&#34;scale_data&#34;, [True]),
            &#34;loss_function&#34;: trial.suggest_categorical(
                &#34;loss_function&#34;,
                [
                    # &#34;FocalTverskyLoss&#34;,
                    # &#34;TverskyLoss&#34;,
                    # &#34;L1Loss&#34;,
                    # &#34;MSELoss&#34;,
                    &#34;HuberLoss&#34;,
                ],
            ),
            &#34;activation&#34;: trial.suggest_categorical(
                &#34;activation&#34;, [&#34;Sigmoid&#34;]  # [&#34;ReLU&#34;, &#34;Sigmoid&#34;, &#34;ELU&#34;]
            ),
            &#34;batch_size&#34;: trial.suggest_int(&#34;batch_size&#34;, 50, 70),
            &#34;weight_decay&#34;: trial.suggest_loguniform(&#34;weight_decay&#34;, 9e-5, 9e-2),
            &#34;dampening&#34;: trial.suggest_loguniform(&#34;dampening&#34;, 1e-1, 7e-1),
            &#34;momentum&#34;: trial.suggest_loguniform(&#34;momentum&#34;, 2e-1, 4e-1),
            &#34;dropout&#34;: trial.suggest_loguniform(&#34;dropout&#34;, 0.2, 0.5),
        }

        for i in range(params[&#34;n_layers&#34;]):
            params[&#34;n_units_layers&#34;].append(
                trial.suggest_int(&#34;n_units_l{}&#34;.format(i), 4, 70)
            )

        loss, accuracy = self.tuneModel(params, trial)

        if self.direction == &#34;maximize&#34;:
            return accuracy
        elif self.direction == &#34;minimize&#34;:
            return loss
        else:
            raise TypeError(&#34;Unknown direction specified&#34;)

    def tuneModel(
        self, params: dict, trial: optuna.trial = None, saveState: bool = False
    ) -&gt; tuple[Union[torch.tensor, float], Union[torch.Tensor, float]]:
        &#34;&#34;&#34;Tunes the model accoring to objective parameters.

        Args:
            params (dict): Paramters generated by Objective.
            trial (optuna.trial): Current optuna trial. Defaults to None.
            saveState (bool): If the parameters should be saved to file. Defaults to False.

        Returns:
            tuple[torch.tensor  float, torch.Tensor  float]: Mean Loss and Accuracy for validation.
        &#34;&#34;&#34;
        print(params)
        try:
            model = Network(
                4, 1, params[&#34;n_layers&#34;], params[&#34;n_units_layers&#34;], p=params[&#34;dropout&#34;]
            )
        except:
            params[&#34;n_units_layers&#34;] = []
            for i in range(params[&#34;n_layers&#34;]):
                params[&#34;n_units_layers&#34;].append(params[f&#34;n_units_l{i}&#34;])

            model = Network(
                4,
                1,
                params[&#34;n_layers&#34;],
                params[&#34;n_units_layers&#34;],
                p=params[&#34;dropout&#34;],
                activation=getattr(torch.nn, params[&#34;activation&#34;]),
            )
        self.trainModel = TrainModel(self.dataPath, model)
        trainLoader, testLoader = self.trainModel.prepareData(
            scale_data=params[&#34;scale_data&#34;],
            batch_size=params[&#34;batch_size&#34;],
            shuffle=True,
        )

        try:
            params[&#34;loss_function&#34;] = getattr(nn, params[&#34;loss_function&#34;])
        except:
            params[&#34;loss_function&#34;] = getattr(losses, params[&#34;loss_function&#34;])

        optim_args = {
            &#34;weight_decay&#34;: params[&#34;weight_decay&#34;],
            &#34;eps&#34;: params[&#34;momentum&#34;],
            # &#34;dampening&#34;: params[&#34;momentum&#34;],
        }

        try:
            train, test = self.trainModel.fit(
                10,
                trainLoader,
                testLoader,
                loss_function=params[&#34;loss_function&#34;],
                optimizer=getattr(torch.optim, params[&#34;optimizer&#34;]),
                optim_args=optim_args,
                learning_rate=params[&#34;learning_rate&#34;],
                trial=trial,
                validate=self.direction,
            )
        except:
            train, test = self.trainModel.fit(
                10,
                trainLoader,
                testLoader,
                loss_function=params[&#34;loss_function&#34;],
                optimizer=getattr(torch.optim, params[&#34;optimizer&#34;]),
                learning_rate=params[&#34;learning_rate&#34;],
                trial=trial,
                validate=self.direction,
            )

        if saveState:
            self.saveBestTrial(params)

        return test[0], test[1]

    def saveBestTrial(self, params: dict, path: Path = PATH):
        &#34;&#34;&#34;Save the state and Parameters of the best trial.

        Args:
            params (optuna.trial.FrozenTrial): The best trial as determined by optuna
            path (Path, optional): Path to saving location. Defaults to PATH.
        &#34;&#34;&#34;
        pathway = Path(os.getcwd() + &#34;/data/models/bestTrial5&#34;)
        if not exists(pathway):
            os.mkdir(pathway)
        self.trainModel.saveInternalStates(pathway)

        if not exists(path / &#34;updatedModelUnscaled&#34;):
            os.mkdir(path / &#34;updatedModelUnscaled&#34;)
        with open(path / &#34;updatedModelUnscaled&#34; / &#34;modelParameters.p&#34;, &#34;wb&#34;) as fp:
            pickle.dump(params, fp, protocol=pickle.HIGHEST_PROTOCOL)

    def saveStudy(self, path: Path):
        &#34;&#34;&#34;Saves the study to the provided path.

        Args:
            path (Path): Path to saving location.
        &#34;&#34;&#34;
        if not exists(path):
            os.mkdir(path)
        with open(path / f&#34;{self.study.study_name}.p&#34;, &#34;wb&#34;) as fp:
            print(f&#34;Saving Study with Name: {self.study.study_name}&#34;)
            pickle.dump(self.study, fp, protocol=pickle.HIGHEST_PROTOCOL)

    def loadStudy(self, path: Path) -&gt; optuna.study.Study:
        &#34;&#34;&#34;Load a study from the provided location.

        Args:
            path (Path): The location of the study.

        Returns:
            optuna.study.Study: The loaded study.
        &#34;&#34;&#34;
        with open(path, &#34;rb&#34;) as file:
            data = pickle.load(file)
        return data

    def __logging_callback(
        self, study: optuna.study.Study, frozen_trial: optuna.trial.FrozenTrial
    ):
        &#34;&#34;&#34;Internal method used to save the current best trial.

        Args:
            study (optuna.study.Study): The current study.
            frozen_trial (optuna.trial.FrozenTrial): The current best trial.
        &#34;&#34;&#34;
        previous_best_value = study.user_attrs.get(&#34;previous_best_value&#34;, None)
        if previous_best_value != study.best_value:
            study.set_user_attr(&#34;previous_best_value&#34;, study.best_value)
            print(
                &#34;Trial {} finished with best value: {} and parameters: {}. &#34;.format(
                    frozen_trial.number,
                    frozen_trial.value,
                    frozen_trial.params,
                )
            )
            self.saveBestTrial(frozen_trial.params)


if __name__ == &#34;__main__&#34;:
    import os

    DATA_PATH = Path(os.getcwd() + os.path.normpath(&#34;/data/all/trainDataTogether.csv&#34;))
    STUDY_PATH = Path(
        os.getcwd() + os.path.normpath(&#34;/data/model/studies/5TrialsScaled&#34;)
    )
    STUDY_LOAD = Path(
        os.getcwd() + os.path.normpath(r&#34;\data\model\studies\50trialsScaled.p&#34;)
    )

    tuner = Tuner(
        DATA_PATH, epochs=1, direction=&#34;minimize&#34;, sampler=optuna.samplers.CmaEsSampler
    )
    best_trial = tuner.optimize(n_trials=50)
    tuner.saveStudy(STUDY_PATH)
    # study = tuner.loadStudy(STUDY_LOAD)

    params = {
        &#34;n_layers&#34;: 3,
        &#34;epochs&#34;: 2,
        &#34;learning_rate&#34;: 0.2505950956626377,
        &#34;optimizer&#34;: &#34;Adamax&#34;,
        &#34;scale_data&#34;: True,
        &#34;loss_function&#34;: &#34;MSELoss&#34;,
        &#34;activation&#34;: &#34;GELU&#34;,
        &#34;batch_size&#34;: 65,
        &#34;weight_decay&#34;: 0.00035351893312748976,
        &#34;dampening&#34;: 0.1721094552252759,
        &#34;momentum&#34;: 0.21608929927906143,
        &#34;dropout&#34;: 0.2535510202298927,
        &#34;n_units_l0&#34;: 27,
        &#34;n_units_l1&#34;: 20,
        &#34;n_units_l2&#34;: 14,
    }
    # tuner.tuneModel(best_trial.params, None, True)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.models.tuner.Tuner"><code class="flex name class">
<span>class <span class="ident">Tuner</span></span>
<span>(</span><span>dataPath: pathlib.Path, epochs: int, direction: Literal['maximize', 'minimize'] = 'maximize', sampler: <module 'optuna.samplers' from 'C:\\Users\\stephan.schumacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\samplers\\__init__.py'> = optuna.samplers._tpe.sampler.TPESampler, pruner: <module 'optuna.pruners' from 'C:\\Users\\stephan.schumacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\pruners\\__init__.py'> = optuna.pruners._hyperband.HyperbandPruner)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to initiate a Model tuning session.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataPath</code></strong> :&ensp;<code>Path</code></dt>
<dd>Path to a data source. May change to DB connection</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of epochs in each Trial run</dd>
<dt>direction (Literal[&quot;minimize&quot;, &quot;maximize&quot;], optional): Direction to optimize. &quot;minimize&quot; optimizes Loss,
&quot;maximize&quot; optimizes Accuracy. Defaults to "maximize".</dt>
<dt><strong><code>sampler</code></strong> :&ensp;<code>optuna.samplers</code>, optional</dt>
<dd>Optuna Sampler Algorythm to use. Defaults to optuna.samplers.TPESampler.</dd>
<dt><strong><code>pruner</code></strong> :&ensp;<code>optuna.pruners</code>, optional</dt>
<dd>Optuna Pruner Algorythm to use. Defaults to optuna.pruners.HyperbandPruner.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>optuna.study</code></dt>
<dd>Optuna Study like session.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tuner:
    def __init__(
        self,
        dataPath: Path,
        epochs: int,
        direction: Literal[&#34;minimize&#34;, &#34;maximize&#34;] = &#34;maximize&#34;,
        sampler: optuna.samplers = optuna.samplers.TPESampler,
        pruner: optuna.pruners = optuna.pruners.HyperbandPruner,
    ) -&gt; optuna.study:
        &#34;&#34;&#34;Class to initiate a Model tuning session.

        Args:
            dataPath (Path): Path to a data source. May change to DB connection
            epochs (int): Number of epochs in each Trial run
            direction (Literal[&amp;quot;minimize&amp;quot;, &amp;quot;maximize&amp;quot;], optional): Direction to optimize. &amp;quot;minimize&amp;quot; optimizes Loss,  &amp;quot;maximize&amp;quot; optimizes Accuracy. Defaults to &#34;maximize&#34;.
            sampler (optuna.samplers, optional): Optuna Sampler Algorythm to use. Defaults to optuna.samplers.TPESampler.
            pruner (optuna.pruners, optional): Optuna Pruner Algorythm to use. Defaults to optuna.pruners.HyperbandPruner.

        Returns:
            optuna.study: Optuna Study like session.
        &#34;&#34;&#34;
        # assigning variables
        self.dataPath = dataPath
        self.epochs = epochs
        self.direction = direction
        # create study
        self.study = optuna.create_study(
            direction=direction,
            sampler=sampler(),
            pruner=pruner(),
            storage=&#34;sqlite:///50Studies_p4.db&#34;,
        )

    def optimize(
        self,
        n_trials: int,
    ) -&gt; optuna.trial.FrozenTrial:
        &#34;&#34;&#34;Starts the optimization process.

        Args:
            n_trials (int): Number of trials to run.

        Returns:
            optuna.trial.FrozenTrial: The best trial according to optimizer.
        &#34;&#34;&#34;
        print(
            f&#34;Starting optimizer session with {n_trials} Trials and {self.epochs} Epochs&#34;
        )
        self.study.optimize(
            self.__objective,
            n_trials=n_trials,
            catch=(RuntimeError, RuntimeWarning, TypeError, ValueError),
            callbacks=[self.__logging_callback],
        )

        return self.study.best_trial

    def __objective(self, trial: optuna.trial) -&gt; float:
        &#34;&#34;&#34;Generate the study objectives.

        Args:
            trial (optuna.trial): The current trial.

        Raises:
            TypeError: Unknown Direction. Shouldn&#39;t occur. Sanity check.

        Returns:
            float: The current objective. Either accuracy or loss
        &#34;&#34;&#34;
        params = {
            &#34;n_layers&#34;: trial.suggest_int(&#34;n_layers&#34;, 1, 5),
            &#34;n_units_layers&#34;: [],
            &#34;learning_rate&#34;: trial.suggest_loguniform(&#34;learning_rate&#34;, 1e-6, 9e-3),
            &#34;optimizer&#34;: trial.suggest_categorical(
                &#34;optimizer&#34;,
                [&#34;Adamax&#34;],  # &#34;ASGD&#34;, &#34;Adam&#34;, &#34;Adamax&#34;]
            ),
            &#34;scale_data&#34;: trial.suggest_categorical(&#34;scale_data&#34;, [True]),
            &#34;loss_function&#34;: trial.suggest_categorical(
                &#34;loss_function&#34;,
                [
                    # &#34;FocalTverskyLoss&#34;,
                    # &#34;TverskyLoss&#34;,
                    # &#34;L1Loss&#34;,
                    # &#34;MSELoss&#34;,
                    &#34;HuberLoss&#34;,
                ],
            ),
            &#34;activation&#34;: trial.suggest_categorical(
                &#34;activation&#34;, [&#34;Sigmoid&#34;]  # [&#34;ReLU&#34;, &#34;Sigmoid&#34;, &#34;ELU&#34;]
            ),
            &#34;batch_size&#34;: trial.suggest_int(&#34;batch_size&#34;, 50, 70),
            &#34;weight_decay&#34;: trial.suggest_loguniform(&#34;weight_decay&#34;, 9e-5, 9e-2),
            &#34;dampening&#34;: trial.suggest_loguniform(&#34;dampening&#34;, 1e-1, 7e-1),
            &#34;momentum&#34;: trial.suggest_loguniform(&#34;momentum&#34;, 2e-1, 4e-1),
            &#34;dropout&#34;: trial.suggest_loguniform(&#34;dropout&#34;, 0.2, 0.5),
        }

        for i in range(params[&#34;n_layers&#34;]):
            params[&#34;n_units_layers&#34;].append(
                trial.suggest_int(&#34;n_units_l{}&#34;.format(i), 4, 70)
            )

        loss, accuracy = self.tuneModel(params, trial)

        if self.direction == &#34;maximize&#34;:
            return accuracy
        elif self.direction == &#34;minimize&#34;:
            return loss
        else:
            raise TypeError(&#34;Unknown direction specified&#34;)

    def tuneModel(
        self, params: dict, trial: optuna.trial = None, saveState: bool = False
    ) -&gt; tuple[Union[torch.tensor, float], Union[torch.Tensor, float]]:
        &#34;&#34;&#34;Tunes the model accoring to objective parameters.

        Args:
            params (dict): Paramters generated by Objective.
            trial (optuna.trial): Current optuna trial. Defaults to None.
            saveState (bool): If the parameters should be saved to file. Defaults to False.

        Returns:
            tuple[torch.tensor  float, torch.Tensor  float]: Mean Loss and Accuracy for validation.
        &#34;&#34;&#34;
        print(params)
        try:
            model = Network(
                4, 1, params[&#34;n_layers&#34;], params[&#34;n_units_layers&#34;], p=params[&#34;dropout&#34;]
            )
        except:
            params[&#34;n_units_layers&#34;] = []
            for i in range(params[&#34;n_layers&#34;]):
                params[&#34;n_units_layers&#34;].append(params[f&#34;n_units_l{i}&#34;])

            model = Network(
                4,
                1,
                params[&#34;n_layers&#34;],
                params[&#34;n_units_layers&#34;],
                p=params[&#34;dropout&#34;],
                activation=getattr(torch.nn, params[&#34;activation&#34;]),
            )
        self.trainModel = TrainModel(self.dataPath, model)
        trainLoader, testLoader = self.trainModel.prepareData(
            scale_data=params[&#34;scale_data&#34;],
            batch_size=params[&#34;batch_size&#34;],
            shuffle=True,
        )

        try:
            params[&#34;loss_function&#34;] = getattr(nn, params[&#34;loss_function&#34;])
        except:
            params[&#34;loss_function&#34;] = getattr(losses, params[&#34;loss_function&#34;])

        optim_args = {
            &#34;weight_decay&#34;: params[&#34;weight_decay&#34;],
            &#34;eps&#34;: params[&#34;momentum&#34;],
            # &#34;dampening&#34;: params[&#34;momentum&#34;],
        }

        try:
            train, test = self.trainModel.fit(
                10,
                trainLoader,
                testLoader,
                loss_function=params[&#34;loss_function&#34;],
                optimizer=getattr(torch.optim, params[&#34;optimizer&#34;]),
                optim_args=optim_args,
                learning_rate=params[&#34;learning_rate&#34;],
                trial=trial,
                validate=self.direction,
            )
        except:
            train, test = self.trainModel.fit(
                10,
                trainLoader,
                testLoader,
                loss_function=params[&#34;loss_function&#34;],
                optimizer=getattr(torch.optim, params[&#34;optimizer&#34;]),
                learning_rate=params[&#34;learning_rate&#34;],
                trial=trial,
                validate=self.direction,
            )

        if saveState:
            self.saveBestTrial(params)

        return test[0], test[1]

    def saveBestTrial(self, params: dict, path: Path = PATH):
        &#34;&#34;&#34;Save the state and Parameters of the best trial.

        Args:
            params (optuna.trial.FrozenTrial): The best trial as determined by optuna
            path (Path, optional): Path to saving location. Defaults to PATH.
        &#34;&#34;&#34;
        pathway = Path(os.getcwd() + &#34;/data/models/bestTrial5&#34;)
        if not exists(pathway):
            os.mkdir(pathway)
        self.trainModel.saveInternalStates(pathway)

        if not exists(path / &#34;updatedModelUnscaled&#34;):
            os.mkdir(path / &#34;updatedModelUnscaled&#34;)
        with open(path / &#34;updatedModelUnscaled&#34; / &#34;modelParameters.p&#34;, &#34;wb&#34;) as fp:
            pickle.dump(params, fp, protocol=pickle.HIGHEST_PROTOCOL)

    def saveStudy(self, path: Path):
        &#34;&#34;&#34;Saves the study to the provided path.

        Args:
            path (Path): Path to saving location.
        &#34;&#34;&#34;
        if not exists(path):
            os.mkdir(path)
        with open(path / f&#34;{self.study.study_name}.p&#34;, &#34;wb&#34;) as fp:
            print(f&#34;Saving Study with Name: {self.study.study_name}&#34;)
            pickle.dump(self.study, fp, protocol=pickle.HIGHEST_PROTOCOL)

    def loadStudy(self, path: Path) -&gt; optuna.study.Study:
        &#34;&#34;&#34;Load a study from the provided location.

        Args:
            path (Path): The location of the study.

        Returns:
            optuna.study.Study: The loaded study.
        &#34;&#34;&#34;
        with open(path, &#34;rb&#34;) as file:
            data = pickle.load(file)
        return data

    def __logging_callback(
        self, study: optuna.study.Study, frozen_trial: optuna.trial.FrozenTrial
    ):
        &#34;&#34;&#34;Internal method used to save the current best trial.

        Args:
            study (optuna.study.Study): The current study.
            frozen_trial (optuna.trial.FrozenTrial): The current best trial.
        &#34;&#34;&#34;
        previous_best_value = study.user_attrs.get(&#34;previous_best_value&#34;, None)
        if previous_best_value != study.best_value:
            study.set_user_attr(&#34;previous_best_value&#34;, study.best_value)
            print(
                &#34;Trial {} finished with best value: {} and parameters: {}. &#34;.format(
                    frozen_trial.number,
                    frozen_trial.value,
                    frozen_trial.params,
                )
            )
            self.saveBestTrial(frozen_trial.params)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="src.models.tuner.Tuner.loadStudy"><code class="name flex">
<span>def <span class="ident">loadStudy</span></span>(<span>self, path: pathlib.Path) ‑> optuna.study.study.Study</span>
</code></dt>
<dd>
<div class="desc"><p>Load a study from the provided location.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code></dt>
<dd>The location of the study.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>optuna.study.Study</code></dt>
<dd>The loaded study.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loadStudy(self, path: Path) -&gt; optuna.study.Study:
    &#34;&#34;&#34;Load a study from the provided location.

    Args:
        path (Path): The location of the study.

    Returns:
        optuna.study.Study: The loaded study.
    &#34;&#34;&#34;
    with open(path, &#34;rb&#34;) as file:
        data = pickle.load(file)
    return data</code></pre>
</details>
</dd>
<dt id="src.models.tuner.Tuner.optimize"><code class="name flex">
<span>def <span class="ident">optimize</span></span>(<span>self, n_trials: int) ‑> optuna.trial._frozen.FrozenTrial</span>
</code></dt>
<dd>
<div class="desc"><p>Starts the optimization process.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_trials</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of trials to run.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>optuna.trial.FrozenTrial</code></dt>
<dd>The best trial according to optimizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize(
    self,
    n_trials: int,
) -&gt; optuna.trial.FrozenTrial:
    &#34;&#34;&#34;Starts the optimization process.

    Args:
        n_trials (int): Number of trials to run.

    Returns:
        optuna.trial.FrozenTrial: The best trial according to optimizer.
    &#34;&#34;&#34;
    print(
        f&#34;Starting optimizer session with {n_trials} Trials and {self.epochs} Epochs&#34;
    )
    self.study.optimize(
        self.__objective,
        n_trials=n_trials,
        catch=(RuntimeError, RuntimeWarning, TypeError, ValueError),
        callbacks=[self.__logging_callback],
    )

    return self.study.best_trial</code></pre>
</details>
</dd>
<dt id="src.models.tuner.Tuner.saveBestTrial"><code class="name flex">
<span>def <span class="ident">saveBestTrial</span></span>(<span>self, params: dict, path: pathlib.Path = WindowsPath('C:/Users/stephan.schumacher/Documents/repos/dh-backend-api/data/models'))</span>
</code></dt>
<dd>
<div class="desc"><p>Save the state and Parameters of the best trial.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>optuna.trial.FrozenTrial</code></dt>
<dd>The best trial as determined by optuna</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Path to saving location. Defaults to PATH.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveBestTrial(self, params: dict, path: Path = PATH):
    &#34;&#34;&#34;Save the state and Parameters of the best trial.

    Args:
        params (optuna.trial.FrozenTrial): The best trial as determined by optuna
        path (Path, optional): Path to saving location. Defaults to PATH.
    &#34;&#34;&#34;
    pathway = Path(os.getcwd() + &#34;/data/models/bestTrial5&#34;)
    if not exists(pathway):
        os.mkdir(pathway)
    self.trainModel.saveInternalStates(pathway)

    if not exists(path / &#34;updatedModelUnscaled&#34;):
        os.mkdir(path / &#34;updatedModelUnscaled&#34;)
    with open(path / &#34;updatedModelUnscaled&#34; / &#34;modelParameters.p&#34;, &#34;wb&#34;) as fp:
        pickle.dump(params, fp, protocol=pickle.HIGHEST_PROTOCOL)</code></pre>
</details>
</dd>
<dt id="src.models.tuner.Tuner.saveStudy"><code class="name flex">
<span>def <span class="ident">saveStudy</span></span>(<span>self, path: pathlib.Path)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the study to the provided path.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code></dt>
<dd>Path to saving location.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveStudy(self, path: Path):
    &#34;&#34;&#34;Saves the study to the provided path.

    Args:
        path (Path): Path to saving location.
    &#34;&#34;&#34;
    if not exists(path):
        os.mkdir(path)
    with open(path / f&#34;{self.study.study_name}.p&#34;, &#34;wb&#34;) as fp:
        print(f&#34;Saving Study with Name: {self.study.study_name}&#34;)
        pickle.dump(self.study, fp, protocol=pickle.HIGHEST_PROTOCOL)</code></pre>
</details>
</dd>
<dt id="src.models.tuner.Tuner.tuneModel"><code class="name flex">
<span>def <span class="ident">tuneModel</span></span>(<span>self, params: dict, trial: <module 'optuna.trial' from 'C:\\Users\\stephan.schumacher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\trial\\__init__.py'> = None, saveState: bool = False) ‑> tuple[typing.Union[<built-in method tensor of type object at 0x00007FFD4FD4E7F0>, float], typing.Union[torch.Tensor, float]]</span>
</code></dt>
<dd>
<div class="desc"><p>Tunes the model accoring to objective parameters.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Paramters generated by Objective.</dd>
<dt><strong><code>trial</code></strong> :&ensp;<code>optuna.trial</code></dt>
<dd>Current optuna trial. Defaults to None.</dd>
<dt><strong><code>saveState</code></strong> :&ensp;<code>bool</code></dt>
<dd>If the parameters should be saved to file. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[torch.tensor
float, torch.Tensor
float]</code></dt>
<dd>Mean Loss and Accuracy for validation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tuneModel(
    self, params: dict, trial: optuna.trial = None, saveState: bool = False
) -&gt; tuple[Union[torch.tensor, float], Union[torch.Tensor, float]]:
    &#34;&#34;&#34;Tunes the model accoring to objective parameters.

    Args:
        params (dict): Paramters generated by Objective.
        trial (optuna.trial): Current optuna trial. Defaults to None.
        saveState (bool): If the parameters should be saved to file. Defaults to False.

    Returns:
        tuple[torch.tensor  float, torch.Tensor  float]: Mean Loss and Accuracy for validation.
    &#34;&#34;&#34;
    print(params)
    try:
        model = Network(
            4, 1, params[&#34;n_layers&#34;], params[&#34;n_units_layers&#34;], p=params[&#34;dropout&#34;]
        )
    except:
        params[&#34;n_units_layers&#34;] = []
        for i in range(params[&#34;n_layers&#34;]):
            params[&#34;n_units_layers&#34;].append(params[f&#34;n_units_l{i}&#34;])

        model = Network(
            4,
            1,
            params[&#34;n_layers&#34;],
            params[&#34;n_units_layers&#34;],
            p=params[&#34;dropout&#34;],
            activation=getattr(torch.nn, params[&#34;activation&#34;]),
        )
    self.trainModel = TrainModel(self.dataPath, model)
    trainLoader, testLoader = self.trainModel.prepareData(
        scale_data=params[&#34;scale_data&#34;],
        batch_size=params[&#34;batch_size&#34;],
        shuffle=True,
    )

    try:
        params[&#34;loss_function&#34;] = getattr(nn, params[&#34;loss_function&#34;])
    except:
        params[&#34;loss_function&#34;] = getattr(losses, params[&#34;loss_function&#34;])

    optim_args = {
        &#34;weight_decay&#34;: params[&#34;weight_decay&#34;],
        &#34;eps&#34;: params[&#34;momentum&#34;],
        # &#34;dampening&#34;: params[&#34;momentum&#34;],
    }

    try:
        train, test = self.trainModel.fit(
            10,
            trainLoader,
            testLoader,
            loss_function=params[&#34;loss_function&#34;],
            optimizer=getattr(torch.optim, params[&#34;optimizer&#34;]),
            optim_args=optim_args,
            learning_rate=params[&#34;learning_rate&#34;],
            trial=trial,
            validate=self.direction,
        )
    except:
        train, test = self.trainModel.fit(
            10,
            trainLoader,
            testLoader,
            loss_function=params[&#34;loss_function&#34;],
            optimizer=getattr(torch.optim, params[&#34;optimizer&#34;]),
            learning_rate=params[&#34;learning_rate&#34;],
            trial=trial,
            validate=self.direction,
        )

    if saveState:
        self.saveBestTrial(params)

    return test[0], test[1]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.models" href="index.html">src.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.models.tuner.Tuner" href="#src.models.tuner.Tuner">Tuner</a></code></h4>
<ul class="">
<li><code><a title="src.models.tuner.Tuner.loadStudy" href="#src.models.tuner.Tuner.loadStudy">loadStudy</a></code></li>
<li><code><a title="src.models.tuner.Tuner.optimize" href="#src.models.tuner.Tuner.optimize">optimize</a></code></li>
<li><code><a title="src.models.tuner.Tuner.saveBestTrial" href="#src.models.tuner.Tuner.saveBestTrial">saveBestTrial</a></code></li>
<li><code><a title="src.models.tuner.Tuner.saveStudy" href="#src.models.tuner.Tuner.saveStudy">saveStudy</a></code></li>
<li><code><a title="src.models.tuner.Tuner.tuneModel" href="#src.models.tuner.Tuner.tuneModel">tuneModel</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>